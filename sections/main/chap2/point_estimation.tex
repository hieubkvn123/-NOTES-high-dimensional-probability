%% ---------------- %%
\subsection{Sufficiency \& Likelihood Principles}
\newcommand{\FacT}{\hyperref[thm:factorisation_theorem]{(\mathrm{{\bf FacT}})}}

\subsubsection{Sufficiency}
\begin{definition}[Sufficient Statistics]
    Let $\bX = (X_1, \dots, X_n) \sim p(.;\btheta)$ be a random sample drawn $i.i.d$ from a distribution with parameters $\btheta$. Let $\bU=T(\bX)$ be a statistic, then it is called a \underline{sufficient statistic} if the conditional distribution $p_{\bX|\bU}$ does not depend on $\btheta$.
\end{definition}

\begin{example}[Bernoulli random variables]
    Let $\bX = (X_1, \dots, X_n) \sim \bern(\theta)$ be a random sample from the Bernoulli distribution. Let $\bU = \frac{1}{n}\sum_{i=1}^n X_i$, then $\bU$ is a sufficient statistic of $\theta$. To illustrate this, suppose that $\bx = (x_1, \dots, x_n)$ is an observation of the random sample $\bX$ and $\bu = \frac{1}{n}\sum_{i=1}^n x_i$. We have:
    \begin{align*}
        \Pm(\bX=\bx| \bU=\bu) &= \frac{\Pm(\bX=\bx, \bU=\bu)}{\Pm(\bU=\bu)} \\
            &= \frac{\Pm(X_1=x_1, \dots, X_n=x_n, \sum_{i=1}^n X_i = \sum_{i=1}^n x_i)}{\Pm(\sum_{i=1}^n X_i = \sum_{i=1}^n x_i)} \\
            &= \frac{\Pm(X_1=x_1, \dots, X_n=x_n)}{\Pm(\sum_{i=1}^n X_i = \sum_{i=1}^n x_i)} \\
            &= \frac{\theta^{\sum_{i=1}^n x_i}(1-\theta)^{n-\sum_{i=1}^n x_i}}{\Pm(\sum_{i=1}^n X_i = \sum_{i=1}^n x_i)}.
    \end{align*}

    \noindent Now, setting $k=\sum_{i=1}^n x_i$, The denominator is basically the probability that the Bernoulli variables sums up to $k$. Hence, we can calculate the denominator as follows:
    \begin{align*}
        \Pm\biggRound{\sum_{i=1}^n X_i = k} &= \begin{pmatrix}
            n \\ k
        \end{pmatrix} \theta^k(1-\theta)^{n - k}.
    \end{align*}

    \noindent Therefore, we have:
    \begin{align*}
        \Pm(\bX = \bx | \bU = \bu) &= \frac{\theta^k(1-\theta)^{n-k}}{
            \begin{pmatrix}
                n \\ k
            \end{pmatrix} \theta^k(1-\theta)^{n - k}
        } = \frac{1}{
            \begin{pmatrix}
                n \\ k
            \end{pmatrix}
        }.
    \end{align*}

    \noindent Therefore, the conditional distribution does not depend on $\theta$ and $\bU$ is a sufficient statistic.
\end{example}

\begin{definition}[Sufficiency Principle]
    If $\bU=T(\bX)$ is a sufficient statistic for $\btheta$, then any inference about $\btheta$ should only depend on the sample $\bX$ through $\bU$. \color{blue}In other words, if we estimate $\btheta$ using an estimator $\hat\btheta$, only $\bU$ shows up in the formula of $\hat\btheta$, not the sample $\bX$ itself. We will see why this is the case in the Factorisation Theorem $\FacT$, which states that we can factorise the density function into a function of $\bU, \btheta$ and a function of the observations $\bx$ and thus, the inference about $\btheta$ is independent of the observations $\bx$\color{black}.
\end{definition}

\begin{theorem}{Factorisation Theorem $\FacT$}{factorisation_theorem}
    Let $\bX = (X_1, \dots, X_n)$ be a random sample with joint density function $p(\bx;\btheta)$ over $\mathcal{X}^n$. The statistic $\bU=T(\bX)$ is sufficient for the parameters $\btheta$ if and only if we can find functions $h, g$ such that:
    \begin{align*}
        p(\bx;\btheta) &= g(T(\bx), \btheta)h(\bx),
    \end{align*}

    \noindent for all $\bx\in\R^n$ and $\btheta\in\Theta$.
\end{theorem}

\begin{proof*}[Factorisation Theorem $\FacT$]
    We have to conduct the proof in both directions.
    \begin{itemize}
        \item $T(\bX)$ is sufficient $\implies$ Factorisation exists:
        Let $\bU=T(\bX)$ be a sufficient statistics and $\bu = T(\bx)$ be the statistics evaluated on the observations $\bx$. Then, we have:
        \begin{align*}
            p(\bx;\btheta) &= \Pm(\bX=\bx; \btheta) \\ 
                &= \Pm(\bX=\bx | \bU=\bu; \btheta) \Pm(\bU=\bu;\btheta).
        \end{align*} 

        \noindent Since $\bU=T(\bX)$ is a sufficient statistics, $\Pm(\BX=\bx|\bU=\bu;\btheta)$ does not depend on $\btheta$. Hence, we denote $h(\bx) = \Pm(\bX=\bx|\bU=\bu;\btheta)$. Furthermore, $\Pm(\bU=\bu;\btheta)$ is a function of $\bu$ and $\btheta$. We denote this function as $g(\bu, \btheta)$ and conclude that the factorisation $p(\bx;\btheta)=h(\bx)g(T(\bx),\btheta)$ indeed exists. 

        \item Factorisation exists $\implies$ $T(\bX)$ is sufficient: Suppose that there exists $g, h$ such that $p(\bx;\btheta)=g(T(\bx), \btheta)h(\bx)$. We then have:
        \begin{align*}
            \Pm(\bX = \bx|\bU = \bu;\btheta) &=  \frac{p(\bx;\btheta)}{\Pm(\bU=\bu;\btheta)} = \frac{g(\bu, \btheta)h(\bx)}{\Pm(\bU=\bu;\btheta)}.
        \end{align*} 

        \noindent We denote $A_\bu = \bigCurl{\tilde\bx \in \mathcal{X}^n : T(\tilde\bx) = \bu}$. We have:
        \begin{align*}
            \Pm(\bU = \bu;\btheta) &= \sum_{\tilde \bx \in A_\bu} \Pm(\bX = \tilde\bx) \\ 
                &= \sum_{\tilde\bx\in A_\bu} p(\tilde\bx;\btheta) = \sum_{\tilde\bx\in A_\bu} g(T(\tilde\bx), \btheta)h(\tilde\bx) \\ 
                &= g(\bu, \btheta) \sum_{\tilde\bx\in A_\bu} h(\tilde\bx).
        \end{align*} 

        \noindent From the above, we have:
        \begin{align*}
            \Pm(\bX=\bx|\bU=\bu;\btheta) &= \frac{h(\bx)}{\sum_{\tilde\bx\in A_\bu} h(\tilde\bx)},
        \end{align*} 
        \noindent and the above expression does not depend on $\btheta$. Hence, $T(\bX)$ is a sufficient statistics.
    \end{itemize}

\subsubsection{Likelihood}
\begin{definition}[Likelihood Function]
    Let $\bX=(X_1, \dots, X_n)$ be a random sample from a distribution $p(.;\theta)$ that depends on parameters $\theta\in\Theta$. Let $\bx=(x_1, \dots, x_n)$ be an observation of the random sample $\bX$. Then, the likelihood function $L(\theta;\bx)$ is defined as follows:
    \begin{equation}
        L(\theta;\bx) = \prod_{i=1}^n p(x_i;\theta), \quad \theta\in\Theta.
    \end{equation}

    \noindent In some cases, we also use the log-likelihood function:
    \begin{equation}
        \ell(\theta;\bx) = \log L(\theta;\bx) = \sum_{i=1}^n \log p(x_i;\theta), \quad \theta\in\Theta.
    \end{equation}

    \noindent Essentially, $L(\theta;\bx)$ quantifies the likelihood that $\theta$ generates the observations $\bx$. In a way, it is the inverse of probability density (mass) functions, we can see the contrast as follows:
    \begin{itemize}
        \item \textbf{Probability Density Function}: The parameters are fixed but the observations are random.
        \item \textbf{Likelihood Function}: The observations are fixed but the parameters are variable.
    \end{itemize} 
\end{definition}

\begin{definition}[Maximum Likelihood Estimator]
    Given $\bX=(X_1, \dots, X_n)$ be a random sample from a distribution $p(.;\theta)$ that depends on $\theta\in\Theta$ and $\bx=(x_1, \dots, x_n)$ be an observation of $\bX$. The \underline{Maximum Likelihood Estimator} $\theta_{MLE}\in\Theta$ is the parameter that maximizes the likelihood function:
    \begin{equation}
        \theta_{MLE} = \arg\max_{\theta\in\Theta} L(\theta;\bx).
    \end{equation}

    \noindent In the subsequent propositions, we will discuss some of the key properties of MLE.
\end{definition} 


\begin{proposition}{Consistency of MLE}{consistency_of_mle}
    Let $\bX=(X_1, \dots, X_n)$ be a random sample from a distribution $p(.;\btheta)$ over $\mathcal{X}$ dependent on a true set of parameters $\btheta$. Let $\Theta$ be the parameters space. Then, the Maximum Likelihood Estimator $\theta_{MLE} = \arg\max_{\theta\in\Theta}L(\theta;\bX)$, which is a random variable, is consistent, meaning $\theta_{MLE}\xrightarrow{p}\btheta$, provided that the following conditions are met:
    \begin{enumerate}
        \item $\btheta\in\Theta$ and $\Theta$ is a compact space. 
        \item $\log p(x;\theta)$ is continuous in $\theta$ for almost all $x\in\mathcal{X}$.
        \item $\E_{\btheta}[\sup_{\theta\in\Theta}|\log(X;\theta)|] < \infty$.
        \item The mapping $\xi\mapsto p(.;\xi), \quad \xi\in\Theta$ is one-to-one (Identifiability).\footnote{In general, it is required that the model is strongly identifiable. However, since the parameters space $\Theta$ is compact, this requirement is satisfied.}
    \end{enumerate} 
    
    \noindent Furthermore, we can also show that $\theta_{MLE}$ is asymptotically unbiased. In other words, $\lim_{n\to\infty}\E[\theta_{MLE}] = \btheta$.
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:consistency_of_mle}]
    A proof for consistency of MLE can be found in \cite[Theorem 2.5]{book:newey1994}. In this section, we briefly go through the proof again.
\end{proof*} 

\begin{proposition}{Asymptotic Normality of MLE}{asymptotic_normality_mle}
    Let $\bX=(X_1, \dots, X_n)$ be a random sample from a distribution $p(.;\btheta)$ dependent on a true set of parameters $\btheta$. Let $\Theta$ be the parameters space. Then, the Maximum Likelihood Estimator $\theta_{MLE} = \arg\max_{\theta\in\Theta}L(\theta;\bX)$ is asymptotically normal:
    \begin{equation}
        \frac{\theta_{MLE} - \btheta}{\sqrt{\var(\theta_{MLE})/n}} \xrightarrow{d} \mathcal{N}(0,1).
    \end{equation}
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:asymptotic_normality_mle}]
    
\end{proof*} 

%% ---------------- %%
\subsection{Point Estimation}
\newcommand{\RB}{\hyperref[thm:rao_blackwell_theorem]{(\mathrm{{\bf RB}})}}
\newcommand{\CRLB}{\hyperref[thm:cramer_rao_lowerbound]{(\mathrm{{\bf CRLB}})}}

\subsubsection{Bias, Variance, Consistency and MSE}

\subsubsection{Sufficient Statistics \& Rao-Blackwell Theorem}
\begin{theorem}{Rao-Blackwell Theorem $\RB$}{rao_blackwell_theorem}
     
\end{theorem}


\subsubsection{Estimator Variance \& Cramer-Rao Lower Bound}
\begin{definition}[Fisher Information]
    Let $\bX=(X_1, \dots, X_n) \sim p(.;\btheta)$ be a random sample from a distribution parameterized by $\btheta$. The \underline{(total) Fisher Information} about $\theta$ in the random sample $\bX$ is defined as follows:
    \begin{equation}
        \mathcal{I}_\bX(\btheta) = \E_\bX\biggSquare{
            \biggRound{
                \frac{\partial}{\partial\theta}\log L(\theta;\bX)
            }^2 \quad \Bigg| \quad \btheta
        }.
    \end{equation}

    \noindent The Fisher Informaton is the total information about $\btheta$ contained in the sample $\bX$.
\end{definition}


\begin{theorem}{Cramer-Rao Lower Bound $\CRLB$}{cramer_rao_lowerbound}

\end{theorem}


\subsubsection{Maximum Likelihood Estimation (MLE)}


