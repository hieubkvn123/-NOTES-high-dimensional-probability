%% ---------------- %%
\subsection{Point Estimation}
\newcommand{\RB}{\hyperref[thm:rao_blackwell_theorem]{(\mathrm{{\bf RB}})}}
\newcommand{\CRLB}{\hyperref[thm:cramer_rao_lowerbound]{(\mathrm{{\bf CRLB}})}}

\subsubsection{Bias and Variance} 


\subsubsection{Consistent Estimator}


\subsubsection{Mean Squared Error (MSE)}

\subsubsection{Rao-Blackwell Theorem}
\begin{theorem}{Rao-Blackwell Theorem $\RB$}{rao_blackwell_theorem}
     
\end{theorem}


\subsubsection{Cramer-Rao Lower Bound}
\begin{definition}[Fisher Information]
    Let $\bX=(X_1, \dots, X_n) \sim p_{\btheta}$ be a random sample from a distribution $p_{\btheta}$ with the true unknown parameter $\btheta$. The \underline{Fisher Information} about $\btheta$ in the random sample $\bX$ is defined as follows:
    \begin{equation}
        \mathcal{I}_\bX(\btheta) = \E_{\btheta}\biggSquare{
            \biggRound{
                \frac{\partial}{\partial\xi}\log L(\xi;\bX)\Bigg|_{\xi=\btheta}
            }^2
        },
    \end{equation}

    \noindent where $\E_{\btheta}$ denotes the expectation conditioned on $\btheta$. The Fisher Information is the total information about $\btheta$ contained in the sample $\bX$.
\end{definition}

\begin{remark}
    Some Notes on Fisher Information:
    \begin{enumerate}
        \item The expectation $\E_\theta$ conditioned on $\theta$ means that the expectation is taken over the sample $\bX$ under the hypothesis that $\theta$ is the true parameter of all the $X_i$'s. 

        \item Inside the expectation is the score function evaluated at $\btheta$.
        \item From two points above, the Fisher Information is essentially calculated as \textbf{the variance conditioned on $\btheta$ of the score evaluated at $\btheta$}.
    \end{enumerate} 
\end{remark} 

\begin{proposition}{Alternative Representation of Fisher Information}{alt_rep_of_fisher}
    Let $\bX=(X_1, \dots, X_n) \sim p_{\btheta}$ be a random sample from a distribution parameterized by a true unknown parameter $\btheta$. Let $\ell(\theta;\bX)=\log L(\theta;\bX)$ be the log-likelihood function. Then, we have:
    \begin{equation}
        \mathcal{I}_{\bX}(\btheta) = \mathrm{Var}(s(\btheta;\bX)), 
    \end{equation} 

    \noindent where $s(\btheta;\bX)=\frac{\partial\ell(\btheta;\bX)}{\partial\btheta}$ is the score evaluated at the true parameter $\btheta$. Furthermore, we have:
    \begin{equation}
        \mathcal{I}_{\bX}(\btheta) = -\E_{\btheta}\biggSquare{\frac{\partial^2}{\partial\xi^2}\ell(\xi;\bX)\Bigg|_{\xi=\btheta}}.
    \end{equation} 
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:alt_rep_of_fisher}]
    Denote $s(\btheta;\bX)$ as the score evaluated at the true parameter $\btheta$, we have:
    \begin{align*}
        \mathrm{Var}(s(\btheta;\bX)) &= \E_{\btheta}\biggSquare{
            \biggRound{
                \frac{\partial}{\partial\xi}\ell(\xi;\bX)\Bigg|_{\xi=\btheta}
            }^2
        } - \E_{\btheta}\biggSquare{
            \frac{\partial}{\partial\xi}\ell(\xi;\bX)\Bigg|_{\xi=\btheta}
        }^2 \\
        &= \mathcal{I}_{\bX}(\btheta) - \E_{\btheta}\biggSquare{
            \frac{1}{n}\sum_{i=1}^n \frac{\partial\log p_\xi(X_i)}{\partial\xi}\Bigg|_{\xi=\btheta}
        } \\
        &= \mathcal{I}_{\bX}(\btheta) - \E_{\btheta}[s(\btheta;X)]^2,
    \end{align*}

    \noindent where $X$ is identically distributed as $X_1, \dots, X_n$. We claim that $\E_{\btheta}[s(\btheta;X)]=0$. Let $\chi$ be the sample space, we have:
    \begin{align*}
        \E_{\btheta}[s(\btheta;X)] &= \int_\chi p_{\btheta}(x) \frac{\partial\log p_\xi(x)}{\partial\xi}\Bigg|_{\xi=\btheta}dx \\
            &= \int_\chi p_{\btheta}(x)\cdot \frac{1}{p_{\btheta}(x)}\frac{\partial p_{\btheta}(x)}{\partial\btheta}dx \\
            &= \int_\chi \frac{\partial p_{\btheta}(x)}{\partial\btheta}dx \\
            &= \frac{\partial}{\partial\btheta}\int_\chi p_{\btheta}(x)dx = \frac{\partial}{\partial\btheta}1 = 0. \qquad \text{(Leibniz's Differentiation Rule)}
    \end{align*} 

    \noindent Therefore, we have $\boxed{\mathcal{I}_{\bX}(\btheta) = \mathrm{Var}(s(\btheta;\bX))}$. Now, we prove the second point: $\mathcal{I}_{\bX}(\btheta) = -\E_{\btheta}\bigSquare{\frac{\partial^2}{\partial\xi^2}\ell(\xi;\bX)\Big|_{\xi=\btheta}}$. Expanding $\frac{\partial^2}{\partial\xi^2}\ell(\xi;\bX)\Big|_{\xi=\btheta}$, we have:
    \begin{align*}
        \frac{\partial^2}{\partial\xi^2}\ell(\xi;\bX)\Big|_{\xi=\btheta} &= \frac{\partial}{\partial\btheta}\biggSquare{\frac{\partial}{\partial\btheta}\ell(\btheta;\bX)} \\
        &= \frac{\partial}{\partial\btheta}\biggSquare{\frac{1}{L(\btheta;\bX)}\frac{\partial}{\partial\btheta}L(\btheta;\bX)} \\
        &= -\frac{1}{L(\btheta;\bX)^2}\biggSquare{\frac{\partial}{\partial\btheta} L(\btheta;\bX)}^2 + \frac{1}{L(\btheta;\bX)}\frac{\partial^2 L(\btheta;\bX)}{\partial\btheta^2} \\
        &= -\biggSquare{\frac{\partial}{\partial\btheta}\ell(\btheta;\bX)}^2 + \frac{1}{L(\btheta;\bX)}\frac{\partial^2 L(\btheta;\bX)}{\partial\btheta^2}.
    \end{align*} 

    \noindent Now, let $P_{\btheta}$ be the probability density function for the random sample $\bX$. Then, essentially, $P_{\btheta}(\bX) = L(\btheta;\bX)$. Therefore, we have:
    \begin{align*}
        \E_{\bX\sim P_{\btheta}} \biggSquare{\frac{1}{L(\btheta;\bX)}\frac{\partial^2 L(\btheta;\bX)}{\partial\btheta^2}} &= \int_{\chi^n} \frac{1}{L(\btheta;\bx)}\frac{\partial^2 L(\btheta;\bx)}{\partial\btheta^2} P_{\btheta}(\bx) d\bx \\
        &= \int_{\chi^n}\frac{\partial^2 L(\btheta;\bx)}{\partial\btheta^2} d\bx \\
        &= \frac{\partial^2}{\partial\btheta^2}\int_{\chi^n} L(\btheta;\bx)d\bx \qquad \text{(Leibniz's Differentiation Rule)} \\
        &= \frac{\partial^2}{\partial\btheta^2} 1 = 0.
    \end{align*} 

    \noindent Therefore, we have:
    \begin{align*}
        -\E_{\btheta}\biggSquare{\frac{\partial^2}{\partial\xi}\ell(\xi;\bX)\Bigg|_{\xi=\btheta}} = \E_{\btheta}\biggSquare{\biggRound{\frac{\partial}{\partial\btheta}\ell(\btheta;\bX)}^2} = \mathcal{I}_{\bX}(\btheta),
    \end{align*} 

    as desired.
\end{proof*} 

\begin{remark}[Exchanging Derivative and Integral]
    Note that in the proof of proposition \ref{prop:alt_rep_of_fisher}, we exchanged the integration and the partial derivative to prove that $\E_{\btheta}[s(\btheta;X)]=0$. This exchange of differentiation and integral can be done under certain conditions, which are listed in the appendix, section \ref{sec:leib_diff_rule}.
\end{remark}

\begin{theorem}{Cramer-Rao Lower Bound $\CRLB$}{cramer_rao_lowerbound}
    Let $\bX = \{X_1, \dots, X_n\}$ be a random sample where $X_i \sim p_{\btheta}$. If $\hat\Theta(\bX)$ is an estimator (biased or unbiased) for $\btheta$ based on the random sample $\bX$, then, we have:
    \begin{equation}
        \mathrm{Var}(\hat\Theta(\bX)) \ge \mathcal{I}_{\bX}(\btheta)^{-1}\biggSquare{\frac{d}{d\btheta}\E[\hat\Theta(\bX)]}^2. 
    \end{equation} 

    \noindent When $\hat\Theta$ is unbiased, we have $\E[\hat\Theta(\bX)] = \btheta$. Hence, we have $\boxed{\mathrm{Var}(\hat\Theta(\bX))\ge\mathcal{I}_{\bX}(\btheta)^{-1}}$.
\end{theorem}

\begin{proof*}[Cramer-Rao Lower Bound $\CRLB$]
    We make use of the following covariance inequality: For two random variables $X, Y$, the following inequality holds
    \begin{equation}
        \mathrm{Cov}(X, Y)^2 \le \mathrm{Var}(X)\cdot\mathrm{Var}(Y).
    \end{equation} 

    \noindent We denote $X, Y$ as follows:
    \begin{align*}
        X = \hat\Theta(\bX); \qquad Y = s(\btheta;\bX). 
    \end{align*} 

    \noindent Applying the above inequality, we have:
    \begin{align*}
        \mathrm{Var}(\hat\Theta(\bX)) \ge \frac{\mathrm{Cov}(\hat\Theta(\bX), s(\btheta;\bX))}{\mathrm{Var}(s(\btheta;\bX))} = \mathcal{I}_{\bX}(\btheta)^{-1}\mathrm{Cov}(\hat\Theta(\bX), s(\btheta;\bX)).
    \end{align*} 

    \noindent Now, we derive the lower bound for $\mathrm{Cov}(\hat\Theta(\bX), s(\btheta;\bX))$. Suppose that the random variables $X_i$ maps from a sample space to $\chi$ and $P_{\btheta}:\chi^n\to [0,1]$ is the probability density for the random sample $\bX$, we have:
    \begin{align*}
        \mathrm{Cov}(\hat\Theta(\bX), s(\btheta;\bX)) &= \E[\hat\Theta(\bX)s(\btheta;\bX)] - \underbrace{\E[\hat\Theta(\bX)]\E[s(\btheta;\bX)]}_{\text{Equals } 0} \\
        &= \int_{\chi^n} \hat\Theta(\bx) s(\btheta;\bx)P_{\btheta}(\bx)d\bx \\
        &= \int_{\chi^n} \hat\Theta(\bx) \frac{d}{d\btheta}\ln L(\btheta;\bx) P_{\btheta}(\bx)d\bx \\
        &= \int_{\chi^n} \hat\Theta(\bx) \frac{\frac{d}{d\btheta}L(\btheta;\bx)}{L(\btheta;\bx)}P_{\btheta}(\bx)d\bx \\
        &= \int_{\chi^n} \hat\Theta(\bx)\frac{d}{d\btheta}L(\btheta;\bx)d\bx \qquad (L(\btheta;\bx) = P_{\btheta}(\bx)) \\
        &= \frac{d}{d\btheta}\int_{\chi^n}\hat\Theta(\bx)L(\btheta;\bx)d\bx \qquad (\text{Leibniz Differentiation Rule}) \\
        &= \frac{d}{d\btheta}\E[\hat\Theta(\bX)].
    \end{align*} 

    \noindent Hence, we obtain the desired lower bound.
\end{proof*} 



