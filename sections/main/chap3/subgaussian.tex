\newcommand{\BTPSG}{\hyperref[thm:bounded_tail_equiv_sg]{(\mathrm{{\bf BTP-SG}})}}
\subsection{Sub-Gaussian Distributions}
A random variable $X$ is called ``sub-Gaussian" if its distribution has a strong tail decay similar to that of a Gaussian distribution. More precisely, the tail of $X$'s probability density/mass function is dominated by the tail of a Gaussian distribution, hence the name ``sub-Gaussian".

\noindent\newline Sub-Gaussian is a particularly interesting family of random variables because by modelling random events using sub-Gaussian distributions, we can bound the probability of very rare events happening at the tail. Furthermore, sub-Gaussianity implies the following properties:
\begin{enumerate}[label=(\roman*)]
	\item Bounded \textbf{tail probability}.
	\item Bounded \textbf{moment}.
	\item Bounded \textbf{moment generating function} (for $X$ and $X^2$).
	\item Bounded \textbf{sub-Gaussian norm\hyperref[sec:subgaussian_norm]{*}}.
\end{enumerate} 

\begin{definition}[Sub-Gaussian Random Variable]
	\label{sec:subgaussian_var}
	A random variable $X$ is called sub-Gaussian if there exists a constant $C>0$ such that:
	\begin{equation}
		\Pm(|X|\ge t) \le 2\exp(-t^2/C^2).	
	\end{equation} 
\end{definition} 

\begin{definition}[Sub-Gaussian Norm]
	\label{sec:subgaussian_norm}
	Let $X$ be a random variable. Then, we denote the sub-Gaussian norm of $X$ as:
	\begin{equation}
		\|X\|_{\psi_2} = \inf\bigCurl{
			t > 0: \E\bigSquare{\exp\bigRound{X^2/t^2}} \le 2
		}.
	\end{equation} 

	\noindent From the above definition and the definition of sub-Gaussian variables, we can deduce that $X$ is sub-Gaussian if and only if $\|X\|_{\psi_2} < \infty$. Furthermore, the intuition behind $\|.\|_{\psi_2}$ norm is that it measures the light-tailed-ness of a random variable. In other words, \textbf{the smaller the sub-Gaussian norm, the faster the tail decays}.
\end{definition} 

\begin{definition}[Sub-Gaussian Variance Proxy]
	Let $X$ be a sub-Gaussian random variable. Then, we say that $X$ is sub-Gaussian with variance proxy $\sigma^2$ if:
	\begin{equation}
		M_X(\lambda) \le \exp(\lambda^2\sigma^2/2), \quad \forall \lambda\in\R.	
	\end{equation} 

	\noindent We denote that $X\in\SG(\sigma^2)$.
\end{definition} 

\begin{proposition}{Properties of Sub-Gaussian Variables}{props_subgaussian}
	Let $X$ be a random variable with mean $\mu$. Then, the following properties are equivalent:

	\begin{enumerate}[label=(\roman*)]
		\item \textbf{Sub-Gaussianity (bounded tail probability)}: There exists $K_1>0$ such that
		\begin{equation}
			\Pm(|X| \ge t) \le 2\exp\biggRound{-\frac{t^2}{K_1^2}}.		
		\end{equation} 	

		\item \textbf{Bounded $p$-moment}: There exists $K_2>0$ such that
		\begin{equation}
			\|X\|_{L^p} = (\E |X|^p)^{1/p} \le K_2\sqrt{p}, \quad \forall p \ge 1.	
		\end{equation} 

		\item \textbf{Bounded MGF (of $X^2$)}: There exists $K_3>0$ such that
		\begin{equation}
			M_{X^2}(\lambda^2) \le \exp(K_3^2\lambda^2), \quad \forall \lambda\in\R \text{ and } |\lambda| < \frac{1}{K_3}.	
		\end{equation} 

		\item \textbf{Bounded MGF (of $X$)}: There exists $K_4>0$ such that
		\begin{equation}
			M_{X-\mu}(\lambda) \le \exp(K_4^2\lambda^2), \quad \forall \lambda\in\R.
		\end{equation}

		\noindent In other words, $X-\mu$ has a variance proxy of $\sigma^2\le 2K_4^2$.

		\item \textbf{Bounded sub-Gaussian norm}: There exists $K_5>0$ such that
		\begin{equation}
			\E\bigSquare{\exp\bigRound{X^2/K_5^2}}\le 2.
		\end{equation} 

		\noindent In other words, $\|X\|_{\psi_2}\le K_5$.
	\end{enumerate} 

	\noindent The parameters $K_1, \dots, K_5$ differ from each other by at most an absolute constant. Meaning, there exists a constant $C$ independent of $K_1, \dots, K_5$ such that $K_i \le C K_j$ for any two $i,j \in \{1, \dots, 5\}$.
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:props_subgaussian}]
	We prove that ${\bf (i)}\implies{\bf (ii)}\implies{\bf (iii)}\implies{\bf (v)}$ then prove that ${\bf (v)}\implies{\bf (i)}$. This means that the statements ${\bf (i)}, {\bf (ii)}, {\bf (iii)}, {\bf (v)}$ are equivalent. Then, we prove that ${\bf (ii)} \iff {\bf (iv)}$. Without loss of generality, assume that $K_1=1$ because we can rescale $X$ to $X/K_1$.
	\begin{enumerate}
		\item ${\bf (i)}\implies{\bf (ii)}$: By the integral identity for $p^{th}$ moments, we have
		\begin{align*}
			\E|X|^p &= \int_0^\infty pt^{p-1}\Pm(|X|>t)dt \\
				&\le 2p\int_0^\infty t^{p-1}e^{-t^2}dt.
		\end{align*}

		\noindent Recall that $\Gamma(z) = \int_0^\infty u^{z-1}e^{-u}du$. By the change of variable, let $u=t^2$ (then, we have $du = 2tdt$). We have:
		\begin{align*}
			\E|X|^p &\le p\int_0^\infty (\sqrt{u})^{p-2}e^{-u}du \\
				&= p\int_0^\infty u^{p/2 - 1}e^{-u}du = p\Gamma(p/2).
		\end{align*} 

		\noindent Since $\Gamma(z) < 3z^z$ for all $z\ge 1/2$, we have $\E|X|^p \le 3p(p/2)^{p/2}$. Taking $p^{th}$ root from both sides, we can have $K_2=3$ since $[3p(p/2)^{p/2}]^{1/p} \le 3\sqrt{p}$.

		\item ${\bf (ii)}\implies{\bf (iii)}$: Using the identity $e^x = 1 + \sum_{k=1}^\infty \frac{x^k}{k!}$, we have:

		\begin{align*}
			M_{X^2}(\lambda^2) &= \E\exp(X^2\lambda^2) \\
				&= \E\biggSquare{
					1 + \sum_{k=1}^\infty \frac{(\lambda^2X^2)^k}{k!}
				} \\
				&= 1 + \sum_{k=1}^\infty \frac{\lambda^{2k} \E[X^{2k}]}{k!}.
		\end{align*} 

		\noindent By property ${\bf (ii)}$, we have $\E[X^{2k}] \le K_2^{2k}(2k)^{k}$. By Stirling's approximation, we have $k!\ge(k/e)^k$. Combining the bounds, we have:
		\begin{align*}
			M_{X^2}(\lambda^2) &\le 1 + \sum_{k=1}^\infty \frac{\lambda^{2k}K_2^{2k}(2k)^k}{(k/e)^k}= 1 + \sum_{k=1}^\infty \frac{(2k\lambda^2K_2^2)^k}{(k/e)^k} = 1 + \sum_{k=1}^\infty (2e\lambda^2K_2^2)^k.
		\end{align*} 

		\noindent When $2e\lambda^2K_2^2 < 1$, we have the sum of geometric series that converges to $\frac{1}{1 - 2e\lambda^2K_2^2}$. Hence, we need $|\lambda| < \frac{1}{K_2\sqrt{2e}}$. Therefore, property $\bf (iii)$ holds with $K_3 = K_2\sqrt{2e}$.

		\item ${\bf (iii)}\implies {\bf (v)}$: Let $K_5=\frac{K_3}{\sqrt{\ln 2}}$ and set $\lambda=\frac{1}{K_5}$. Since $K_5^{-1} < \frac{1}{K_3}$, statement $\bf (iii)$ holds with $\lambda=K_5^{-1}$. Therefore:
		\begin{align*}
			\E\exp\bigSquare{X^2/K_5^2} &\le \exp(K_3^2/K_5^2) = \exp(\ln 2) = 2.
		\end{align*} 

		\item ${\bf (v)}\implies{\bf (i)}$: Let $K_5>0$ be the constant that makes $\bf (v)$ hold. We have
		\begin{align*}
			\Pm(|X| \ge t) &= \Pm(X^2\ge t^2) \\
				&= \Pm\bigRound{e^{X^2/K_5^2} \ge e^{t^2/K_5^2}} \\
				&\le \exp\biggRound{-\frac{t^2}{K_5^2}}\E\bigSquare{e^{X^2/K_5^2}} \quad \text{(Markov's Inequality)} \\
				&\le 2\exp\biggRound{-\frac{t^2}{K_5^2}}.
		\end{align*} 

		\noindent Hence, $\bf (i)$ holds with $K_1\ge K_5$. From here on, we have proved that the statements $\bf (i), (ii), (iii)$ and $\bf (v)$ are equivalent. 

		\item ${\bf (i)} \iff {\bf (iv)}$: This relationship is proven below in theorem \ref{thm:bounded_tail_equiv_sg}.
	\end{enumerate} 
\end{proof*} 

\begin{theorem}{Bounded Tail Probability $\iff \mathcal{SG}$ $\BTPSG$}{bounded_tail_equiv_sg}
	Let $X$ be a random variable with mean $\E[X] = \mu$ and $Z$ be the centered random variable $Z=X-\mu$. Then, for all $t\in\R$,
	\begin{enumerate}[label=(\roman*)]
		\item $Z\in\mathcal{SG}(\sigma^2) \implies \Pm ({|X-\mu|\ge t}) \le 2\exp\bigRound{-\frac{t^2}{2\sigma^2}}$ for some $\sigma>0$.
		\item $\Pm ({|X-\mu|\ge t}) \le 2\exp\bigRound{-\frac{t^2}{2\xi^2}} \implies Z\in\mathcal{SG}(16\xi^2)$ for some $\xi>0$.
	\end{enumerate} 
\end{theorem} 

\begin{proof*}[Theorem $\BTPSG$]
	We prove each bullet point one by one.
	\begin{enumerate}[label=(\roman*)]
		\item From the definition, for all $\lambda\in\R$, we have $M_Z(\lambda) = \E\exp[\lambda Z] \le \exp\bigRound{\frac{\lambda^2\sigma^2}{2}}$. For $\lambda>0$ and for all $t>0$, we have:
		\begin{align*}
			\Pm(|X-\mu| \ge t) &= \Pm\bigRound{\bigCurl{X - \mu \ge t} \vee \bigCurl{X - \mu \le -t}} \\
			&\le \Pm(Z \ge t) + \Pm(Z\le -t) \\
			&= \Pm(e^{\lambda Z} \ge e^{\lambda t}) + \Pm(Z \le -t)
		\end{align*} 

		\noindent Then, we have the following inequalities:
		\begin{align*}
			\begin{cases}
				\Pm(e^{\lambda Z} \ge e^{\lambda t}) &\le e^{-\lambda t}M_Z(\lambda) \qquad \text{(Markov's Inequality)} \\
				\Pm(Z \le -t) &\le e^{-\lambda t} M_Z(-\lambda) \qquad \text{(Chernoff's Bound)}
			\end{cases},
		\end{align*} 

		\noindent where the second inequality comes from the left-tail Chernoff's Bound\footnote{$\Pm(X\le a)\le e^{-sa}M_X(s)$ for all $s<0$. Hence, $\Pm(Z\le -t)\le e^{st}M_Z(s)$ for all $s<0$. Setting $s=-\lambda$, we have $\Pm(Z\le -t) \le e^{-\lambda t}M_Z(-\lambda)$.}. As a result, we have:
		\begin{align*}
			\Pm(|X-\mu|\ge t) &\le e^{-\lambda t}\bigSquare{M_Z(\lambda) + M_Z(-\lambda)} \\
				&\le 2e^{-\lambda t} \exp\biggRound{\frac{\lambda^2\sigma^2}{2}} \\
				&= 2\exp\biggRound{-\lambda t + \frac{\lambda^2\sigma^2}{2}}.
		\end{align*} 

		\noindent Using Lagrange multiplier to solve for $\lambda$, we have $\lambda=\frac{t}{\sigma^2}$. Hence, we have:
		\begin{align*}
			\Pm(|X-\mu| \ge t) \le 2\exp\biggRound{-\frac{t^2}{2\sigma^2}}.	
		\end{align*} 

		\item Firstly, we prove that bounded tail probability implies that $\E|Z|^{2q} \le q!(4\xi^2)^q$ for all integers $q\ge1$. Using the identity $\E|Z|^q = \int_0^\infty qt^{q-1}\Pm(|Z|\ge t)dt$, we have:
	    \begin{align*}
	        \E|Z|^{2q} &= 2q\int_0^\infty t^{2q-1}\Pm(|Z|\ge t)dt \\
	            &\le 4q\int_0^\infty t^{2q-1}\exp\biggRound{-\frac{t^2}{2\xi^2}}dt.
	    \end{align*}

	    \noindent Letting $u=\frac{t^2}{2\xi^2}$, hence $t^2 = 2u\xi^2$ and $dt = \frac{\xi^2du}{t}$, the above integral becomes:
	    \begin{align*}
	        \E|Z|^{2q} &\le 4q\xi^2\int_0^\infty t^{2q-2}e^{-u}du \\
	            &= 4q\xi^2\int_{0}^\infty (2u\xi^2)^{q-1}e^{-u}du \\
	            &= 2q\cdot(2\xi^2)^q \underbrace{\int_0^\infty u^{q-1}e^{-u}du}_{\Gamma(q)} \\
	            &= 2q!(2\xi^2)^q \le q!(4\xi^2)^q.
	    \end{align*}

	    \noindent Let $\tilde Z$ be the $i.i.d$ copy of $Z$. Hence, $Z-\tilde Z$ is symmetric about $0$, which means that $\E[(Z-\tilde Z)^p] = 0$ for odd-order $p$-moments. Therefore, For all $\lambda>0$, we have:
	    \begin{align*}
	        M_Z(\lambda)M_{-\tilde Z}(\lambda) &= M_{Z-\tilde Z}(\lambda) \quad (\text{Due to independence})\\
	            &= \E\exp\bigRound{\lambda(Z - \tilde Z)} \\
	            &= 1 + \sum_{q=1}^\infty \frac{\lambda^{2q}\E\bigSquare{(Z - \tilde Z)^{2q}}}{(2q)!}.
	    \end{align*}

	    \noindent By the convexity of $f(z) = z^{2q}$, for all $t\in(0,1)$, we have:
	    \begin{align*}
	        \bigSquare{tZ + (1-t)(-\tilde Z)}^{2q} \le tZ^{2q} + (1-t)\tilde Z^{2q}.
	    \end{align*}

	    \noindent Setting $t=\frac{1}{2}$, we have:
	    \begin{align*}
	        \biggSquare{\frac{Z - \tilde Z}{2}}^{2q} \le \frac{Z^{2q} + \tilde Z^{2q}}{2} \implies (Z - \tilde Z)^{2q} \le 2^{2q-1}(Z^{2q} + \tilde Z^{2q}).
	    \end{align*}

	    \noindent As a result, we have $\E[(Z-\tilde Z)^{2q}] \le 2^{2q-1}(\E[Z^{2q}] + \E[\tilde Z^{2q}]) = 2^{2q}\E[Z^{2q}]$. Plugging this back to the formula of $M_{Z-\tilde Z}(\lambda)$, we have:
	    \begin{align*}
	        \E[e^{\lambda Z}]\E[e^{-\lambda \tilde Z}] &\le 1 + \sum_{q=1}^\infty \frac{\lambda^{2q}2^{2q}\E[Z^{2q}]}{(2q)!} \\
	        &\le 1 + \sum_{q=1}^\infty \frac{\lambda^{2q}2^{2q}(4\xi^2)^{q}q!}{(2q)!}.
	    \end{align*}

	    \noindent Since $\E[e^{-\lambda \tilde Z}]\ge 1$ for all $\lambda>0$ and 
	    \begin{align*}
	        \frac{(2q)!}{q!} &= \prod_{j=1}^q (q+j) \ge \prod_{j=1}^q (2j) = 2^qq!,
	    \end{align*}

	    \noindent We have:
	    \begin{align*}
	        \E[e^{\lambda Z}] &\le 1 + \sum_{q=1}^\infty \frac{(2\lambda^2\cdot 4\xi^2)^q}{q!} = e^{8\lambda^2\xi^2}.
	    \end{align*}

	    \noindent Hence, $Z\in\mathcal{SG}(16\xi^2)$.
	\end{enumerate}	 
\end{proof*} 
