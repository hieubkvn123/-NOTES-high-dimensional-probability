\subsection{Sub-Gaussian Distributions}
A random variable $X$ is called ``sub-Gaussian" if its distribution has a strong tail decay similar to that of a Gaussian distribution. More precisely, the tail of $X$'s probability density/mass function is dominated by the tail of a Gaussian distribution, hence the name ``sub-Gaussian".

\noindent\newline Sub-Gaussian is a particularly interesting family of random variables because by modelling random events using sub-Gaussian distributions, we can bound the probability of very rare events happening at the tail. Furthermore, sub-Gaussianity implies the following properties:
\begin{enumerate}[label=(\roman*)]
	\item Bounded \textbf{tail probability}.
	\item Bounded \textbf{moment}.
	\item Bounded \textbf{moment generating function} (for $X$ and $X^2$).
	\item Bounded \textbf{sub-Gaussian norm\hyperref[sec:subgaussian_norm]{*}}.
\end{enumerate} 

\begin{definition}[Sub-Gaussian Random Variable]
	\label{sec:subgaussian_var}
	A random variable $X$ is called sub-Gaussian if there exists a constant $C>0$ such that:
	\begin{equation}
		\Pm(|X|\ge t) \le 2\exp(-t^2/C^2).	
	\end{equation} 
\end{definition} 

\begin{definition}[Sub-Gaussian Norm]
	\label{sec:subgaussian_norm}
	Let $X$ be a random variable. Then, we denote the sub-Gaussian norm of $X$ as:
	\begin{equation}
		\|X\|_{\psi_2} = \inf\bigCurl{
			t > 0: \E\bigSquare{\exp\bigRound{X^2/t^2}} \le 2
		}.
	\end{equation} 

	\noindent From the above definition and the definition of sub-Gaussian variables, we can deduce that $X$ is sub-Gaussian if and only if $\|X\|_{\psi_2} < \infty$. Furthermore, the intuition behind $\|.\|_{\psi_2}$ norm is that it measures the light-tailed-ness of a random variable. In other words, \textbf{the smaller the sub-Gaussian norm, the faster the tail decays}.
\end{definition} 

\begin{definition}[Sub-Gaussian Variance Proxy]
	Let $X$ be a sub-Gaussian random variable. Then, we say that $X$ is sub-Gaussian with variance proxy $\sigma^2$ if:
	\begin{equation}
		M_X(\lambda) \le \exp(\lambda^2\sigma^2/2), \quad \forall \lambda\in\R.	
	\end{equation} 

	\noindent We denote that $X\in\SG(\sigma^2)$.
\end{definition} 

\begin{proposition}{Properties of Sub-Gaussian Variables}{props_subgaussian}
	Let $X$ be a random variable with mean $\mu$. Then, the following properties are equivalent:

	\begin{enumerate}[label=(\roman*)]
		\item \textbf{Sub-Gaussianity (bounded tail probability)}: There exists $K_1>0$ such that
		\begin{equation}
			\Pm(|X| \ge t) \le 2\exp\biggRound{-\frac{t^2}{K_1^2}}.		
		\end{equation} 	

		\item \textbf{Bounded $p$-moment}: There exists $K_2>0$ such that
		\begin{equation}
			\|X\|_{L^p} = (\E |X|^p)^{1/p} \le K_2\sqrt{p}, \quad \forall p \ge 1.	
		\end{equation} 

		\item \textbf{Bounded MGF (of $X^2$)}: There exists $K_3>0$ such that
		\begin{equation}
			M_{X^2}(\lambda^2) \le \exp(K_3^2\lambda^2), \quad \forall \lambda\in\R \text{ and } |\lambda| < \frac{1}{K_3}.	
		\end{equation} 

		\item \textbf{Bounded MGF (of $X$)}: There exists $K_4>0$ such that
		\begin{equation}
			M_{X-\mu}(\lambda) \le \exp(K_4^2\lambda^2), \quad \forall \lambda\in\R.
		\end{equation}

		\noindent In other words, $X-\mu$ has a variance proxy of $\sigma^2\le 2K_4^2$.

		\item \textbf{Bounded sub-Gaussian norm}: There exists $K_5>0$ such that
		\begin{equation}
			\E\bigSquare{\exp\bigRound{X^2/K_5^2}}\le 2.
		\end{equation} 

		\noindent In other words, $\|X\|_{\psi_2}\le K_5$.
	\end{enumerate} 

	\noindent The parameters $K_1, \dots, K_5$ differ from each other by at most an absolute constant. Meaning, there exists a constant $C$ independent of $K_1, \dots, K_5$ such that $K_i \le C K_j$ for any two $i,j \in \{1, \dots, 5\}$.
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:props_subgaussian}]
	We prove that ${\bf (i)}\implies{\bf (ii)}\implies{\bf (iii)}\implies{\bf (v)}$ then prove that ${\bf (v)}\implies{\bf (i)}$. This means that the statements ${\bf (i)}, {\bf (ii)}, {\bf (iii)}, {\bf (v)}$ are equivalent. Then, we prove that ${\bf (ii)} \iff {\bf (iv)}$. Without loss of generality, assume that $K_1=1$ because we can rescale $X$ to $X/K_1$.
	\begin{enumerate}
		\item ${\bf (i)}\implies{\bf (ii)}$: By the integral identity for $p^{th}$ moments, we have
		\begin{align*}
			\E|X|^p &= \int_0^\infty pt^{p-1}\Pm(|X|>t)dt \\
				&\le 2p\int_0^\infty t^{p-1}e^{-t^2}dt.
		\end{align*}

		\noindent Recall that $\Gamma(z) = \int_0^\infty u^{z-1}e^{-u}du$. By the change of variable, let $u=t^2$ (then, we have $du = 2tdt$). We have:
		\begin{align*}
			\E|X|^p &\le p\int_0^\infty (\sqrt{u})^{p-2}e^{-u}du \\
				&= p\int_0^\infty u^{p/2 - 1}e^{-u}du = p\Gamma(p/2).
		\end{align*} 

		\noindent Since $\Gamma(z) < 3z^z$ for all $z\ge 1/2$, we have $\E|X|^p \le 3p(p/2)^{p/2}$. Taking $p^{th}$ root from both sides, we can have $K_2=3$ since $[3p(p/2)^{p/2}]^{1/p} \le 3\sqrt{p}$.

		\item ${\bf (ii)}\implies{\bf (iii)}$: Using the identity $e^x = 1 + \sum_{k=1}^\infty \frac{x^k}{k!}$, we have:

		\begin{align*}
			M_{X^2}(\lambda^2) &= \E\exp(X^2\lambda^2) \\
				&= \E\biggSquare{
					1 + \sum_{k=1}^\infty \frac{(\lambda^2X^2)^k}{k!}
				} \\
				&= 1 + \sum_{k=1}^\infty \frac{\lambda^{2k} \E[X^{2k}]}{k!}.
		\end{align*} 

		\noindent By property ${\bf (ii)}$, we have $\E[X^{2k}] \le K_2^{2k}(2k)^{k}$. By Stirling's approximation, we have $k!\ge(k/e)^k$. Combining the bounds, we have:
		\begin{align*}
			M_{X^2}(\lambda^2) &\le 1 + \sum_{k=1}^\infty \frac{\lambda^{2k}K_2^{2k}(2k)^k}{(k/e)^k}= 1 + \sum_{k=1}^\infty \frac{(2k\lambda^2K_2^2)^k}{(k/e)^k} = 1 + \sum_{k=1}^\infty (2e\lambda^2K_2^2)^k.
		\end{align*} 

		\noindent When $2e\lambda^2K_2^2 < 1$, we have the sum of geometric series that converges to $\frac{1}{1 - 2e\lambda^2K_2^2}$. Hence, we need $|\lambda| < \frac{1}{K_2\sqrt{2e}}$. Therefore, property $\bf (iii)$ holds with $K_3 = K_2\sqrt{2e}$.

		\item ${\bf (iii)}\implies {\bf (v)}$: Let $K_5=\frac{K_3}{\sqrt{\ln 2}}$ and set $\lambda=\frac{1}{K_5}$. Since $K_5^{-1} < \frac{1}{K_3}$, statement $\bf (iii)$ holds with $\lambda=K_5^{-1}$. Therefore:
		\begin{align*}
			\E\exp\bigSquare{X^2/K_5^2} &\le \exp(K_3^2/K_5^2) = \exp(\ln 2) = 2.
		\end{align*} 

		\item ${\bf (v)}\implies{\bf (i)}$: Let $K_5>0$ be the constant that makes $\bf (v)$ hold. We have
		\begin{align*}
			\Pm(|X| \ge t) &= \Pm(X^2\ge t^2) \\
				&= \Pm\bigRound{e^{X^2/K_5^2} \ge e^{t^2/K_5^2}} \\
				&\le \exp\biggRound{-\frac{t^2}{K_5^2}}\E\bigSquare{e^{X^2/K_5^2}} \quad \text{(Markov's Inequality)} \\
				&\le 2\exp\biggRound{-\frac{t^2}{K_5^2}}.
		\end{align*} 

		\noindent Hence, $\bf (i)$ holds with $K_1\ge K_5$. From here on, we have proved that the statements $\bf (i), (ii), (iii)$ and $\bf (v)$ are equivalent. 

		\item ${\bf (iii)} \iff {\bf (iv)}$: Assume that $\bf (iii)$ holds with constant $K_3>0$. Using the inequality $e^x\le x + e^{x^2}$, we have
		\begin{align*}
			M_{X-\mu}(\lambda) &= \E\exp\bigSquare{\lambda(X-\mu)} \\
				&\le \E\bigSquare{\lambda(X-\mu) + e^{\lambda^2(X-\mu)^2}}
		\end{align*} 
	\end{enumerate} 
\end{proof*} 
