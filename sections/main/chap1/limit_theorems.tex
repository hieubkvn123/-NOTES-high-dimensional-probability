\subsection{Limit Theorems}
%% Link text %%
\newcommand{\SLLN}{\hyperref[thm:strong_law_of_large_numbers]{(\mathrm{{\bf SLLN}})}}
\newcommand{\WLLN}{\hyperref[thm:weak_law_of_large_numbers]{(\mathrm{{\bf WLLN}})}}
\newcommand{\LCT}{\hyperref[thm:levy_continuity_theorem]{(\mathrm{{\bf LCT}})}}
\newcommand{\BCL}{\hyperref[thm:borel_cantelli_lemma]{(\mathrm{{\bf BCL}})}}
\newcommand{\CLT}{\hyperref[thm:central_limit_theorem]{(\mathrm{{\bf CLT}})}}

\subsubsection{Weak Law of Large Numbers}
\begin{theorem}{Weak Law of Large Numbers $\WLLN$}{weak_law_of_large_numbers}
    Let $X_1, \dots, X_N$ be $i.i.d$ random variables with mean $\mu$. Consider the sum:
    \begin{align*}
        S_N = X_1 + \dots + X_N
    \end{align*}

    \noindent Then, the sample mean \textbf{converges to $\mu$ in probability} ($S_N/N\xrightarrow{p}\mu$):
    \begin{align}
        \lim_{N\to\infty}\Pm\bigRound{
            |S_N/N - \mu| > \epsilon
        } = 0, \ \forall \epsilon > 0
    \end{align}
\end{theorem}

\begin{proof*}[Weak Law of Large Numbers $\WLLN$]
    We split the proof into two sections corresponding to the assumptions of finite variance and non-finite variance.
    \begin{enumerate}
        \item \textbf{Finite variance case}:
        Suppose that $\var X_i = \sigma^2<\infty$ for all $1\le i \le N$. Let $\bar X = S_N/N$. Then, $\bar X$ is a random variable with the following mean and variance:
        \begin{align*}
            \E\bar X &= \mu \quad \text{ and } \quad \var \bar X = \frac{\sigma^2}{N}.
        \end{align*}

        \noindent Hence, by the Chebyshev's inequality, we have:
        \begin{align*}
            \Pm\bigRound{|S_N/N - \mu| > \epsilon} &= \Pm\bigRound{|\bar X - \mu| > \epsilon} \le \frac{\sigma^2}{N\epsilon^2}.
        \end{align*}

        \noindent Therefore, we have:
        \begin{align*}
            \lim_{N\to\infty}\Pm\bigRound{|S_N/N - \mu| > \epsilon} &\le \lim_{N\to\infty}\frac{\sigma^2}{N\epsilon^2} = 0.
        \end{align*}

        \noindent Hence, we have $\lim_{N\to\infty}\Pm\bigRound{|S_N/N - \mu| > \epsilon}=0$ and we obtained $\WLLN$.

        \item \textbf{Non-finite variance case}: In this case, we rely on the Levy Continuity Theorem $\LCT$, which relies on the convergence of the characteristic function. For $n\ge1$, define the sequence of random variable $Y_n=S_n/n$. Hence, we have:
        \begin{align*}
            \varphi_{Y_n}(t) &= \varphi_{S_n/n}(t) \\
                &= \varphi_{S_n}(t/n) \\
                &= \prod_{i=1}^n \varphi_{X_i}(t/n) = \bigSquare{\varphi_X(t/n)}^n,
        \end{align*}

        \noindent Where $X=X_1=\dots=X_n$. By Taylor's expansion, we have:
        \begin{align*}
            \varphi_X(t/n) &= 1 + \frac{it\E[X]}{n} + \mathcal{O}(1/n^2) = 1 +\frac{it\mu}{n} + \mathcal{O}(1/n^2).
        \end{align*}

        \noindent Hence, we have:
        \begin{align*}
            \lim_{n\to\infty}\varphi_{Y_n}(t) &= \lim_{n\to\infty}\biggRound{
                1 + \frac{it\mu}{n} + \mathcal{O}(1/n^2)
            }^n = e^{it\mu}.
        \end{align*}

        \noindent Therefore, by $\LCT$, we have $Y_n\xrightarrow{p} \mu$.
    \end{enumerate}

    \begin{remark}[Taylor expansion of Moment Generating and Characteristic Functions]
        Given a random variable $X$. For reference, the following are the Taylor expansions of the Moment Generating Function $M_X(t)$ and the Characteristic Function $\varphi_X(t)$:
        \begin{equation}
            \begin{aligned}
                M_X(t) &= \E[e^{tX}] = 1 + \sum_{n=1}^\infty \frac{t^n}{n!}\E[X^n], \\
                \varphi_X(t) &= \E[e^{itX}] = 1 + \sum_{n=1}^\infty \frac{(it)^n}{n!}\E[X^n].
            \end{aligned}
        \end{equation}

        For the sake of my laziness, here are the Taylor expansion for the first three terms of both the MGF and the CF:
        \begin{equation}
            \begin{aligned}
                M_X(t) &= 1 + t\E[X] + \frac{t^2}{2}\E[X^2] + \mathcal{O}(t^3), \\
                \varphi_X(t) &= 1 + it\E[X] - \frac{t^2}{2}\E[X^2] + \mathcal{O}(t^3).
            \end{aligned}
        \end{equation}
    \end{remark}
\end{proof*}

\begin{theorem}{Levy Continuity Theorem $\LCT$}{levy_continuity_theorem}
    Let $X_1, X_2, \dots$ be $i.i.d$ random variables. Then:
    \begin{equation}
        \forall t \in \R: \lim_{n\to\infty}\varphi_{X_n}(t) = \varphi_X(t) \iff X_n \xrightarrow{d} X,
    \end{equation}

    \noindent for some random variable $X$. In a special case where $X = c$ for some $c\in \R$, we have:
    \begin{equation}
        \forall t \in \R : \lim_{n\to\infty} \varphi_{X_n}(t) = e^{itc} \iff X_n \xrightarrow{p} c.
    \end{equation}
\end{theorem}

\begin{proof*}[Levy Continuity Theorem $\LCT$]
    The proof for $\LCT$ can be found in \cite[Section 9.1, Theorem 9.1 and Collorary 9.1]{book:allen2004}
\end{proof*}

\subsubsection{Strong Law of Large Numbers}
\begin{theorem}{Strong Law of Large Numbers $\SLLN$}{strong_law_of_large_numbers}
    Let $X_1, \dots, X_N$ be $i.i.d$ random variables with mean $\mu$. Consider the sum:
    \begin{align*}
        S_N = X_1 + \dots + X_N
    \end{align*}

    \noindent Then, the sample mean \textbf{converges to $\mu$ almost surely} ($S_N/N\xrightarrow{a.s}\mu$):
    \begin{align}
        \Pm\biggRound{
            \limsup_{N\to\infty}|S_N/N - \mu| > \epsilon
        } = 0, \ \forall \epsilon > 0
    \end{align}
\end{theorem}

\begin{proof*}[Strong Law of Large Numbers $\SLLN$]
    For the sake of simplicity, we will present the proof for $\SLLN$ with an additional assumption that $\E[|X_n|^4] < \infty, \forall n \ge 1$. The proof for the general case of $\SLLN$ (also called the Kolmogorov Strong Law) can be found in \cite[Section 6, Theorem 6.1]{book:allen2004}. For convenience, we assume the following:
    \begin{enumerate}
        \item $\E[|X_n|^4] = K < \infty$.
        \item $\E[X_n] = 0$. For non-zero mean case, we can set $Y_n = X_n - \mu$ and repeat the same arguments made below.
    \end{enumerate}

    \noindent We aim to prove that $\Pm\bigRound{\limsup_{N\to\infty}|S_N/N|>\epsilon} = 0$ for any $\epsilon>0$. Firstly, use the Multinomial formula to expand $\E[S_n]$. The expansion will contain the terms in the following forms:
    \begin{align*}
        X_i^2, X_i^3 X_j, X_i^2X_j^2, X_i^2X_jX_k, X_iX_jX_kX_\ell,
    \end{align*}

    \noindent where $i, j, k, \ell$ are distinct indices. By independence, we have:
    \begin{align*}
        \E[X_i^3X_j] = \E[X_i^2X_jX_k] = \E[X_iX_lX_kX_\ell] = 0.
    \end{align*}

    \noindent As a result, we have the following remaining terms by the Multinomial formula:
    \begin{align*}
        \E[S_n^4] &= \sum_{i=1}^n \E[X_i^4] + \begin{pmatrix}
            4 \\ 2
        \end{pmatrix}\sum_{1 \le i < j \le n}\E[X_i^2X_j^2] \\
        &= \sum_{i=1}^n \E[X_i^4] + 6 \underbrace{\sum_{1 \le i < j \le n}\E[X_i^2X_j^2]}_{n(n-1)/2 \text{ terms}} \\
        &= nK + 3n(n-1)\E[X_i^2X_j^2].
    \end{align*}

    \noindent By independence, we have $\E[X_i^2X_j^2]=\E[X_i^2]\E[X_j^2]$ and for any $1\le i \le n$. Furthermore, we have $\E[X_i^2] = \var(X_i) + \mu^2 = \sigma^2 + \mu^2$. Therefore:
    \begin{align*}
        \E[S_n^4] &= nK + 3n(n-1)(\sigma^2 + \mu^2) < nK + 3n^2(\sigma^2 + \mu^2).
    \end{align*}

    \noindent Applying Markov's Inequality with the fourth moment, we have:
    \begin{align*}
        \Pm(|S_n/n|\ge \epsilon) &= \Pm(|S_n|\ge n\epsilon) \\ 
        &\le \frac{\E[S_n^4]}{n^4\epsilon^4} \\
        &< \frac{K}{n^3\epsilon^4} + \frac{3(\sigma^2 + \mu^2)}{n^2}.
    \end{align*}

    \noindent Therefore, we have:
    \begin{align}
        \sum_{n=1}^\infty \Pm(|S_n/n|\ge\epsilon) < \frac{K}{\epsilon^4}\sum_{n=1}^\infty n^{-3} + 3(\sigma^2 + \mu^2)\sum_{n=1}^\infty n^{-2} < \infty
    \end{align}

    \noindent Finally, by the Borel-Cantelli Lemma $\BCL$, we have:
    \begin{align*}
        \Pm\biggRound{\limsup_{n\to\infty} |S_n/n|\ge \epsilon} = 0, \quad \forall \epsilon > 0.
    \end{align*}
\end{proof*}

\begin{theorem}{Borel-Cantelli Lemma $\BCL$}{borel_cantelli_lemma}
    \textbf{1. First Borel-Cantelli Lemma}: Given a probability space $(X, \mathcal{S}, \Pm)$ and a sequence $\{A_n\}_{n=1}^\infty\subset\mathcal{S}$. If $\sum_{n=1}^\infty\Pm(A_n) < \infty$, we have:
    \begin{equation}
        \Pm\biggRound{\limsup_{n\to\infty} A_n} = 0.
    \end{equation}

    \noindent\textbf{2. Second Borel-Cantelli Lemma}: On the other hand, if $\sum_{n=1}^\infty \Pm(E_n) = \infty$, we have:
    \begin{equation}
        \Pm\biggRound{\limsup_{n\to\infty} A_n} = 1.
    \end{equation}
\end{theorem}

\begin{proof*}[Borel-Cantelli Lemma $\BCL$]
    We focus on proving the first Borel-Cantelli lemma. We define another sequence of $\mathcal{S}$-measurable sets $\{B_n\}_{n=1}^\infty$ such that:
    \begin{align*}
        B_n &= \bigcup_{k=n}^\infty A_n.
    \end{align*}

    \noindent Hence, we have $B_{\ell+1}\subset B_\ell$ for every $\ell\ge 1$. In other words, $B_n$ is a decreasing sequence of $\mathcal{S}$-measurable sets. By continuity of measure, we have:
    \begin{align*}
        \Pm\bigRound{\lim_{n\to\infty} B_n} &= \lim_{n\to\infty} \Pm(B_n) \\
            &= \lim_{n\to\infty} \sum_{k=n}^\infty \Pm(A_n) \quad \text{(By additivity)} \\
            &= \sum_{i=1}^\infty\Pm(A_i) - \lim_{n\to\infty} \sum_{k=1}^n \Pm(A_n) \\
            &= 0.
    \end{align*}

    \noindent Furthermore, we have:
    \begin{align*}
        \Pm\bigRound{\lim_{n\to\infty} B_n} &= \Pm\biggRound{\lim_{n\to\infty} \bigcup_{k=n}^\infty} = \Pm\biggRound{\bigcap_{n=1}^\infty \bigcup_{k=n}^\infty A_n} = \Pm\biggRound{\limsup_{n\to\infty} A_n}.
    \end{align*}

    \noindent Hence proved the first Borel-Cantelli Lemma. To prove the second Borel-Cantelli Lemma, we prove the following:
    \begin{align*}
        1 - \Pm\biggRound{\limsup_{n\to\infty} A_n} &= \Pm\biggRound{\bigCurl{\limsup_{n\to\infty} A_n}^c} \\
        &= \Pm\biggRound{\liminf_{n\to\infty} A_n^c} = 0.
    \end{align*}
\end{proof*}


\subsubsection{Uniform Law of Large Number}



\subsubsection{Central Limit Theorem}
\begin{theorem}{Central Limit Theorem $\CLT$}{central_limit_theorem}
    Let $X_1, \dots X_n$ be a sequence of $i.i.d$ random variables with expected value $\mu$ and finite variance $\sigma^2$. Then, we have:
    \begin{equation}
        \frac{\bar X_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1) \quad as \quad n \to \infty,
    \end{equation}

    \noindent where $\bar X_n = S_n/n$ and $\mathcal{N}(0,1)$ is the standard normal distribution.
\end{theorem}

\begin{proof*}[Central Limit Theorem $\CLT$]
    We prove this via the Characteristic Function. Let $\bar Z_n = \frac{\bar X_n - \mu}{\sigma/\sqrt n}$, notice that:
    \begin{align*}
        \bar Z_n &= \frac{\bar X_n - \mu}{\sigma/\sqrt n} = \frac{1}{\sqrt n}\sum_{i=1}^n \frac{X_i - \mu}{\sigma},
    \end{align*}

    \noindent Let $Z_i = X_i - \mu$ for $1\le i \le n$ and suppose $Z = Z_1 = \dots = Z_n$, we have:
    \begin{align*}
        \varphi_{\bar Z_n}(t) &= \varphi_{\sum_{i=1}^n Z_i}\biggRound{\frac{t}{\sqrt n}} = \biggSquare{
            \varphi_Z\biggRound{\frac{t}{\sqrt n}}
        }^n \\
        &= \biggSquare{
            1 + \frac{it\E[Z]}{\sqrt n} - \frac{t^2}{2n}\E[Z^2] + \mathcal{O}(1/n)
        }^n \quad \text{(Taylor's Expansion)} \\
        &= \biggSquare{
            1 - \frac{t^2}{2n} + \mathcal{O}(1/n)
        }^n. 
    \end{align*}

    \noindent The final equality comes from the fact that $\E[Z] = 0$ and $\E[Z^2] = \E[Z]^2 + \var(Z) = 1$. Finally, we have:
    \begin{align*}
        \lim_{n\to\infty}\varphi_{\bar Z_n}(t) = \lim_{n\to\infty} \biggSquare{
            1 - \frac{t^2}{2n} + \mathcal{O}(1/n)
        }^n = e^{-t^2/2}.
    \end{align*}

    \noindent Since $e^{-t^2/2}$ is the Characteristic Function of the standard normal distribution, by $\LCT$, we have $\bar Z_n \xrightarrow{d} \mathcal{N}(0,1)$.
\end{proof*}

