\subsection{Limit Theorems}
%% Link text %%
\newcommand{\SLLN}{\hyperref[thm:strong_law_of_large_numbers]{(\mathrm{{\bf SLLN}})}}
\newcommand{\WLLN}{\hyperref[thm:weak_law_of_large_numbers]{(\mathrm{{\bf WLLN}})}}
\newcommand{\LCT}{\hyperref[thm:levy_continuity_theorem]{(\mathrm{{\bf LCT}})}}

\subsubsection{Weak Law of Large Numbers}
\begin{theorem}{Weak Law of Large Numbers $\WLLN$}{weak_law_of_large_numbers}
    Let $X_1, \dots, X_N$ be $i.i.d$ random variables with mean $\mu$. Consider the sum:
    \begin{align*}
        S_N = X_1 + \dots + X_N
    \end{align*}

    \noindent Then, the sample mean \textbf{converges to $\mu$ in probability} ($S_N/N\xrightarrow{p}\mu$):
    \begin{align}
        \lim_{N\to\infty}\Pm\bigRound{
            |S_N/N - \mu| > \epsilon
        } = 0, \ \forall \epsilon > 0
    \end{align}
\end{theorem}

\begin{proof*}[Weak Law of Large Numbers $\WLLN$]
    We split the proof into two sections corresponding to the assumptions of finite variance and non-finite variance.
    \begin{enumerate}
        \item \textbf{Finite variance case}:
        Suppose that $\var X_i = \sigma^2<\infty$ for all $1\le i \le N$. Let $\bar X = S_N/N$. Then, $\bar X$ is a random variable with the following mean and variance:
        \begin{align*}
            \E\bar X &= \mu \quad \text{ and } \quad \var \bar X = \frac{\sigma^2}{N}.
        \end{align*}

        \noindent Hence, by the Chebyshev's inequality, we have:
        \begin{align*}
            \Pm\bigRound{|S_N/N - \mu| > \epsilon} &= \Pm\bigRound{|\bar X - \mu| > \epsilon} \le \frac{\sigma^2}{N\epsilon^2}.
        \end{align*}

        \noindent Therefore, we have:
        \begin{align*}
            \lim_{N\to\infty}\Pm\bigRound{|S_N/N - \mu| > \epsilon} &\le \lim_{N\to\infty}\frac{\sigma^2}{N\epsilon^2} = 0.
        \end{align*}

        \noindent Hence, we have $\lim_{N\to\infty}\Pm\bigRound{|S_N/N - \mu| > \epsilon}=0$ and we obtained $\WLLN$.

        \item \textbf{Non-finite variance case}: In this case, we rely on the Levy Continuity Theorem $\LCT$, which relies on the convergence of the characteristic function. For $n\ge1$, define the sequence of random variable $Y_n=S_n/n$. Hence, we have:
        \begin{align*}
            \varphi_{Y_n}(t) &= \varphi_{S_n/n}(t) \\
                &= \varphi_{S_n}(t/n) \\
                &= \prod_{i=1}^n \varphi_{X_i}(t/n) = \bigSquare{\varphi_X(t/n)}^n,
        \end{align*}

        \noindent Where $X=X_1=\dots=X_n$. By Taylor's expansion, we have:
        \begin{align*}
            \varphi_X(t/n) &= 1 + \frac{it\E[X]}{n} + \mathcal{O}(1/n^2) = 1 +\frac{it\mu}{n} + \mathcal{O}(1/n^2).
        \end{align*}

        \noindent Hence, we have:
        \begin{align*}
            \lim_{n\to\infty}\varphi_{Y_n}(t) &= \lim_{n\to\infty}\biggRound{
                1 + \frac{it\mu}{n} + \mathcal{O}(1/n^2)
            }^n = e^{it\mu}.
        \end{align*}

        \noindent Therefore, by $\LCT$, we have $Y_n\xrightarrow{p} \mu$.
    \end{enumerate}

    \begin{remark}[Taylor expansion of Moment Generating and Characteristic Functions]
        Given a random variable $X$. For reference, the following are the Taylor expansions of the Moment Generating Function $M_X(t)$ and the Characteristic Function $\varphi_X(t)$:
        \begin{equation}
            \begin{aligned}
                M_X(t) &= \E[e^{tX}] = 1 + \sum_{n=1}^\infty \frac{t^n}{n!}\E[X^n], \\
                \varphi_X(t) &= \E[e^{itX}] = 1 + \sum_{n=1}^\infty \frac{(it)^n}{n!}\E[X^n].
            \end{aligned}
        \end{equation}
    \end{remark}
\end{proof*}

\begin{theorem}{Levy Continuity Theorem $\LCT$}{levy_continuity_theorem}
    Let $X_1, X_2, \dots$ be $i.i.d$ random variables. Then:
    \begin{equation}
        \forall t \in \R: \lim_{n\to\infty}\varphi_{X_n}(t) = \varphi_X(t) \iff X_n \xrightarrow{d} X,
    \end{equation}

    \noindent for some random variable $X$. In a special case where $X = c$ for some $c\in \R$, we have:
    \begin{equation}
        \forall t \in \R : \lim_{n\to\infty} \varphi_{X_n}(t) = e^{itc} \iff X_n \xrightarrow{p} c.
    \end{equation}
\end{theorem}

\begin{proof*}[Levy Continuity Theorem $\LCT$]
    The proof for $\LCT$ can be found in \cite[Section 9.1, Theorem 9.1 and Collorary 9.1]{book:allen2004}
\end{proof*}

\subsubsection{Strong Law of Large Numbers}
\begin{theorem}{Strong Law of Large Numbers $\SLLN$}{strong_law_of_large_numbers}
    Let $X_1, \dots, X_N$ be $i.i.d$ random variables with mean $\mu$. Consider the sum:
    \begin{align*}
        S_N = X_1 + \dots + X_N
    \end{align*}

    \noindent Then, the sample mean \textbf{converges to $\mu$ almost surely} ($S_N/N\xrightarrow{a.s}\mu$):
    \begin{align}
        \Pm\biggRound{
            \limsup_{N\to\infty}|S_N/N - \mu| > \epsilon
        } = 0, \ \forall \epsilon > 0
    \end{align}
\end{theorem}

\begin{proof*}[Strong Law of Large Numbers $\SLLN$]
    For the sake of simplicity, we will present the proof for $\SLLN$ with an additional assumption that $\E[|X_n|^4] < \infty, \forall n \ge 1$. The proof for the general case of $\SLLN$ (also called the Kolmogorov Strong Law) can be found in \cite[Section 6, Theorem 6.1]{book:allen2004}. For convenience, we assume the following:
    \begin{enumerate}
        \item $\E[|X_n|^4] = K < \infty$.
        \item $\E[X_n] = 0$. For non-zero mean case, we can set $Y_n = X_n - \mu$ and repeat the same arguments made below.
    \end{enumerate}

    \noindent Then, use the Binomial formula to expand $\E[S_n]$. The expansion will contain the terms in the following forms:
    \begin{align*}
        X_i^2, X_i^3 X_j, X_i^2X_j^2, X_i^2X_jX_k, X_iX_jX_kX_\ell,
    \end{align*}

    \noindent where $i, j, k, \ell$ are distinct indices. By independence, we have:
    \begin{align*}
        \E[X_i^3X_j] = \E[X_i^2X_j] = \E[X_iX_lX_kX_\ell] = 0.
    \end{align*}

    \noindent As a result, we have the following remaining terms by the Binomial formula:
    \begin{align*}
        \E[S_n^4]
    \end{align*}
\end{proof*}


\subsubsection{Central Limit Theorem}
