\subsection{Limit Theorems}
%% Link text %%
\newcommand{\SLLN}{\hyperref[thm:strong_law_of_large_numbers]{(\mathrm{{\bf SLLN}})}}
\newcommand{\WLLN}{\hyperref[thm:weak_law_of_large_numbers]{(\mathrm{{\bf WLLN}})}}
\newcommand{\ULLN}{\hyperref[thm:uniform_law_of_large_numbers]{(\mathrm{{\bf ULLN}})}}
\newcommand{\LCT}{\hyperref[thm:levy_continuity_theorem]{(\mathrm{{\bf LCT}})}}
\newcommand{\BCL}{\hyperref[thm:borel_cantelli_lemma]{(\mathrm{{\bf BCL}})}}
\newcommand{\CLT}{\hyperref[thm:central_limit_theorem]{(\mathrm{{\bf CLT}})}}

\subsubsection{Weak Law of Large Numbers}
\begin{theorem}{Weak Law of Large Numbers $\WLLN$}{weak_law_of_large_numbers}
    Let $X_1, \dots, X_N$ be $i.i.d$ random variables with mean $\mu$. Consider the sum:
    \begin{align*}
        S_N = X_1 + \dots + X_N
    \end{align*}

    \noindent Then, the sample mean \textbf{converges to $\mu$ in probability} ($S_N/N\xrightarrow{p}\mu$):
    \begin{align}
        \lim_{N\to\infty}\Pm\bigRound{
            |S_N/N - \mu| > \epsilon
        } = 0, \ \forall \epsilon > 0
    \end{align}
\end{theorem}

\begin{proof*}[Weak Law of Large Numbers $\WLLN$]
    We split the proof into two sections corresponding to the assumptions of finite variance and non-finite variance.
    \begin{enumerate}
        \item \textbf{Finite variance case}:
        Suppose that $\var X_i = \sigma^2<\infty$ for all $1\le i \le N$. Let $\bar X = S_N/N$. Then, $\bar X$ is a random variable with the following mean and variance:
        \begin{align*}
            \E\bar X &= \mu \quad \text{ and } \quad \var \bar X = \frac{\sigma^2}{N}.
        \end{align*}

        \noindent Hence, by the Chebyshev's inequality, we have:
        \begin{align*}
            \Pm\bigRound{|S_N/N - \mu| > \epsilon} &= \Pm\bigRound{|\bar X - \mu| > \epsilon} \le \frac{\sigma^2}{N\epsilon^2}.
        \end{align*}

        \noindent Therefore, we have:
        \begin{align*}
            \lim_{N\to\infty}\Pm\bigRound{|S_N/N - \mu| > \epsilon} &\le \lim_{N\to\infty}\frac{\sigma^2}{N\epsilon^2} = 0.
        \end{align*}

        \noindent Hence, we have $\lim_{N\to\infty}\Pm\bigRound{|S_N/N - \mu| > \epsilon}=0$ and we obtained $\WLLN$.

        \item \textbf{Non-finite variance case}: In this case, we rely on the Levy Continuity Theorem $\LCT$, which relies on the convergence of the characteristic function. For $n\ge1$, define the sequence of random variable $Y_n=S_n/n$. Hence, we have:
        \begin{align*}
            \varphi_{Y_n}(t) &= \varphi_{S_n/n}(t) \\
                &= \varphi_{S_n}(t/n) \\
                &= \prod_{i=1}^n \varphi_{X_i}(t/n) = \bigSquare{\varphi_X(t/n)}^n,
        \end{align*}

        \noindent Where $X=X_1=\dots=X_n$. By Taylor's expansion, we have:
        \begin{align*}
            \varphi_X(t/n) &= 1 + \frac{it\E[X]}{n} + \mathcal{O}(1/n^2) = 1 +\frac{it\mu}{n} + \mathcal{O}(1/n^2).
        \end{align*}

        \noindent Hence, we have:
        \begin{align*}
            \lim_{n\to\infty}\varphi_{Y_n}(t) &= \lim_{n\to\infty}\biggRound{
                1 + \frac{it\mu}{n} + \mathcal{O}(1/n^2)
            }^n = e^{it\mu}.
        \end{align*}

        \noindent Therefore, by $\LCT$, we have $Y_n\xrightarrow{p} \mu$.
    \end{enumerate}

    \begin{remark}[Taylor expansion of Moment Generating and Characteristic Functions]
        Given a random variable $X$. For reference, the following are the Taylor expansions of the Moment Generating Function $M_X(t)$ and the Characteristic Function $\varphi_X(t)$:
        \begin{equation}
            \begin{aligned}
                M_X(t) &= \E[e^{tX}] = 1 + \sum_{n=1}^\infty \frac{t^n}{n!}\E[X^n], \\
                \varphi_X(t) &= \E[e^{itX}] = 1 + \sum_{n=1}^\infty \frac{(it)^n}{n!}\E[X^n].
            \end{aligned}
        \end{equation}

        For the sake of my laziness, here are the Taylor expansion for the first three terms of both the MGF and the CF:
        \begin{equation}
            \begin{aligned}
                M_X(t) &= 1 + t\E[X] + \frac{t^2}{2}\E[X^2] + \mathcal{O}(t^3), \\
                \varphi_X(t) &= 1 + it\E[X] - \frac{t^2}{2}\E[X^2] + \mathcal{O}(t^3).
            \end{aligned}
        \end{equation}
    \end{remark}
\end{proof*}

\begin{theorem}{Levy Continuity Theorem $\LCT$}{levy_continuity_theorem}
    Let $X_1, X_2, \dots$ be $i.i.d$ random variables. Then:
    \begin{equation}
        \forall t \in \R: \lim_{n\to\infty}\varphi_{X_n}(t) = \varphi_X(t) \iff X_n \xrightarrow{d} X,
    \end{equation}

    \noindent for some random variable $X$. In a special case where $X = c$ for some $c\in \R$, we have:
    \begin{equation}
        \forall t \in \R : \lim_{n\to\infty} \varphi_{X_n}(t) = e^{itc} \iff X_n \xrightarrow{p} c.
    \end{equation}
\end{theorem}

\begin{proof*}[Levy Continuity Theorem $\LCT$]
    The proof for $\LCT$ can be found in \cite[Section 9.1, Theorem 9.1 and Collorary 9.1]{book:allen2004}
\end{proof*}

\subsubsection{Strong Law of Large Numbers}
\begin{theorem}{Strong Law of Large Numbers $\SLLN$}{strong_law_of_large_numbers}
    Let $X_1, \dots, X_N$ be $i.i.d$ random variables with mean $\mu<\infty$. Consider the sum:
    \begin{align*}
        S_N = X_1 + \dots + X_N
    \end{align*}

    \noindent Then, the sample mean \textbf{converges to $\mu$ almost surely} ($S_N/N\xrightarrow{a.s}\mu$):
    \begin{align}
        \Pm\biggRound{
            \limsup_{N\to\infty}|S_N/N - \mu| > \epsilon
        } = 0, \ \forall \epsilon > 0
    \end{align}
\end{theorem}

\begin{proof*}[Strong Law of Large Numbers $\SLLN$]
    For the sake of simplicity, we will present the proof for $\SLLN$ with an additional assumption that $\E[|X_n|^4] < \infty, \forall n \ge 1$. The proof for the general case of $\SLLN$ (also called the Kolmogorov Strong Law) can be found in \cite[Section 6, Theorem 6.1]{book:allen2004}. For convenience, we assume the following:
    \begin{enumerate}
        \item $\E[|X_n|^4] = K < \infty$.
        \item $\E[X_n] = 0$. For non-zero mean case, we can set $Y_n = X_n - \mu$ and repeat the same arguments made below.
    \end{enumerate}

    \noindent We aim to prove that $\Pm\bigRound{\limsup_{N\to\infty}|S_N/N|>\epsilon} = 0$ for any $\epsilon>0$. Firstly, use the Multinomial formula to expand $\E[S_n]$. The expansion will contain the terms in the following forms:
    \begin{align*}
        X_i^2, X_i^3 X_j, X_i^2X_j^2, X_i^2X_jX_k, X_iX_jX_kX_\ell,
    \end{align*}

    \noindent where $i, j, k, \ell$ are distinct indices. By independence, we have:
    \begin{align*}
        \E[X_i^3X_j] = \E[X_i^2X_jX_k] = \E[X_iX_lX_kX_\ell] = 0.
    \end{align*}

    \noindent As a result, we have the following remaining terms by the Multinomial formula:
    \begin{align*}
        \E[S_n^4] &= \sum_{i=1}^n \E[X_i^4] + \begin{pmatrix}
            4 \\ 2
        \end{pmatrix}\sum_{1 \le i < j \le n}\E[X_i^2X_j^2] \\
        &= \sum_{i=1}^n \E[X_i^4] + 6 \underbrace{\sum_{1 \le i < j \le n}\E[X_i^2X_j^2]}_{n(n-1)/2 \text{ terms}} \\
        &= nK + 3n(n-1)\E[X_i^2X_j^2].
    \end{align*}

    \noindent By independence, we have $\E[X_i^2X_j^2]=\E[X_i^2]\E[X_j^2]$ and for any $1\le i \le n$. Furthermore, we have $\E[X_i^2] = \var(X_i) + \mu^2 = \sigma^2 + \mu^2$. Therefore:
    \begin{align*}
        \E[S_n^4] &= nK + 3n(n-1)(\sigma^2 + \mu^2) < nK + 3n^2(\sigma^2 + \mu^2).
    \end{align*}

    \noindent Applying Markov's Inequality with the fourth moment, we have:
    \begin{align*}
        \Pm(|S_n/n|\ge \epsilon) &= \Pm(|S_n|\ge n\epsilon) \\ 
        &\le \frac{\E[S_n^4]}{n^4\epsilon^4} \\
        &< \frac{K}{n^3\epsilon^4} + \frac{3(\sigma^2 + \mu^2)}{n^2}.
    \end{align*}

    \noindent Therefore, we have:
    \begin{align}
        \sum_{n=1}^\infty \Pm(|S_n/n|\ge\epsilon) < \frac{K}{\epsilon^4}\sum_{n=1}^\infty n^{-3} + 3(\sigma^2 + \mu^2)\sum_{n=1}^\infty n^{-2} < \infty
    \end{align}

    \noindent Finally, by the Borel-Cantelli Lemma $\BCL$, we have:
    \begin{align*}
        \Pm\biggRound{\limsup_{n\to\infty} |S_n/n|\ge \epsilon} = 0, \quad \forall \epsilon > 0.
    \end{align*}
\end{proof*}

\begin{theorem}{Borel-Cantelli Lemma $\BCL$}{borel_cantelli_lemma}
    \textbf{1. First Borel-Cantelli Lemma}: Given a probability space $(X, \mathcal{S}, \Pm)$ and a sequence $\{A_n\}_{n=1}^\infty\subset\mathcal{S}$. If $\sum_{n=1}^\infty\Pm(A_n) < \infty$, we have:
    \begin{equation}
        \Pm\biggRound{\limsup_{n\to\infty} A_n} = 0.
    \end{equation}

    \noindent\textbf{2. Second Borel-Cantelli Lemma}: On the other hand, if $\sum_{n=1}^\infty \Pm(E_n) = \infty$, we have:
    \begin{equation}
        \Pm\biggRound{\limsup_{n\to\infty} A_n} = 1.
    \end{equation}
\end{theorem}

\begin{proof*}[Borel-Cantelli Lemma $\BCL$]
    We focus on proving the first Borel-Cantelli lemma. We define another sequence of $\mathcal{S}$-measurable sets $\{B_n\}_{n=1}^\infty$ such that:
    \begin{align*}
        B_n &= \bigcup_{k=n}^\infty A_n.
    \end{align*}

    \noindent Hence, we have $B_{\ell+1}\subset B_\ell$ for every $\ell\ge 1$. In other words, $B_n$ is a decreasing sequence of $\mathcal{S}$-measurable sets. By continuity of measure, we have:
    \begin{align*}
        \Pm\bigRound{\lim_{n\to\infty} B_n} &= \lim_{n\to\infty} \Pm(B_n) \\
            &= \lim_{n\to\infty} \sum_{k=n}^\infty \Pm(A_n) \quad \text{(By additivity)} \\
            &= \sum_{i=1}^\infty\Pm(A_i) - \lim_{n\to\infty} \sum_{k=1}^n \Pm(A_n) \\
            &= 0.
    \end{align*}

    \noindent Furthermore, we have:
    \begin{align*}
        \Pm\bigRound{\lim_{n\to\infty} B_n} &= \Pm\biggRound{\lim_{n\to\infty} \bigcup_{k=n}^\infty} = \Pm\biggRound{\bigcap_{n=1}^\infty \bigcup_{k=n}^\infty A_n} = \Pm\biggRound{\limsup_{n\to\infty} A_n}.
    \end{align*}

    \noindent Hence proved the first Borel-Cantelli Lemma. To prove the second Borel-Cantelli Lemma, we prove the following:
    \begin{align*}
        1 - \Pm\biggRound{\limsup_{n\to\infty} A_n} &= \Pm\biggRound{\bigCurl{\limsup_{n\to\infty} A_n}^c} \\
        &= \Pm\biggRound{\liminf_{n\to\infty} A_n^c} = 0.
    \end{align*}
\end{proof*}


\subsubsection{Uniform Law of Large Numbers}
The Uniform Law of Large Numbers $\ULLN$ provides a convergence result for collection of estimators where the convergence is uniform in the parameters space.

\begin{theorem}{Uniform Law of Large Numbers $\ULLN$}{uniform_law_of_large_numbers}
    Let $\bX=(X_1, \dots, X_n)$ be a random sample from a distribution $p_\btheta$ over $\mathcal{X}$ that depends on the true parameters $\btheta$. Let $\Theta\subset\R^m$ be the parameters space and $f_\theta:\mathcal{X}\to\R$ be a function indexed by $\theta\in\Theta$ that satisfies the following conditions:
    \begin{enumerate}
        \item $\btheta\in\Theta$ and $\Theta$ is compact.
        \item $\E_{\btheta}[\sup_{\theta\in\Theta}|f_\theta(X)|] < \infty$\footnote{$\E_\btheta$ denotes the expectation taken over the distribution described by $p_\btheta$.}.
        \item $f_\theta(x)$ is continuous in $\theta$ for all $x\in\mathcal{X}$.
    \end{enumerate}

    \noindent Then, we have:
    \begin{equation}
        \sup_{\theta\in\Theta}\biggAbs{
            \frac{1}{n}\sum_{i=1}^n f_\theta(X_i) - \E_{\btheta}[f_\theta(X)]
        } \xrightarrow{p} 0.
    \end{equation}
\end{theorem} 

\begin{proof*}[Theorem \ref{thm:uniform_law_of_large_numbers}]
    A nice proof is provided in \cite[Theorem 16, Page 111]{book:ferguson1996}. However, we will conduct our own version of the proof relying on results in learning theory. For $\theta\in\Theta$, define the following function:
    \begin{equation}
        \phi_\theta(\bX) = \biggAbs{
            \frac{1}{n}\sum_{i=1}^n \bigRound{f_\theta(X_i) - \E_{\btheta}[f_\theta(X)]}
        } 
    \end{equation}

    \noindent We have to prove that $\Pm(\sup_{\theta\in\Theta}|\phi_\theta(\bX)|\ge \epsilon)\to 0$ for all $\epsilon>0$. For $\epsilon>0$, we have:
    \begin{align*}
        \Pm\bigRound{\sup_{\theta\in\Theta}|\phi_\theta(\bX)| \ge \epsilon} 
        &= \Pm\bigRound{\bigCurl{\sup_{\theta\in\Theta} \phi_\theta(\bX) \ge \epsilon}\cup\bigCurl{\sup_{\theta\in\Theta} (-\phi_\theta(\bX)) \ge \epsilon}} \\ 
        &\le \Pm\bigRound{\sup_{\theta\in\Theta} \phi_\theta(\bX) \ge \epsilon} + \Pm\bigRound{\sup_{\theta\in\Theta} (-\phi_\theta(\bX)) \ge \epsilon} \\ 
        &\le \frac{1}{\epsilon}\bigRound{\E\bigSquare{\sup_{\theta\in\Theta}\phi_\theta(\bX)} + \E\bigSquare{\sup_{\theta\in\Theta}(-\phi_\theta(\bX))}} \quad \text{(Markov's Inequality)}
    \end{align*} 

    \noindent Now, we need to bound the expectations on the right-hand-side. To do so, we use the symmetrization trick. Specifically, given a sequence of $i.i.d$ random variables $S = \{Z_1, \dots, Z_n\}\sim\rho^n$ sampled from a distribution $\rho$ and a class of functions $\mathcal{F}$. Let $S'=\{Z_1', \dots, Z_n'\}\sim\rho^n$ be a another sample from the same distribution $\rho$ (which we called the phantom sample), we have:
    \begin{equation}
    \label{eq:ulln_eq1}
    \begin{aligned}
        &\E_S\biggSquare{\sup_{f\in\mathcal{F}}\frac{1}{n}{\sum_{i=1}^n (f(Z_i) - \E_S[f(Z_i)])}} \\
            &= \E_S\biggSquare{\sup_{f\in\mathcal{F}}\frac{1}{n}{\sum_{i=1}^n (f(Z_i) - \E_{S'}[f(Z_i')])}} \\ 
            &= \E_S\biggSquare{\sup_{f\in\mathcal{F}}{
                \frac{1}{n}\sum_{i=1}^n f(Z_i) - \frac{1}{n}\sum_{i=1}^n \E_{S'}[f(Z_i')]
            }} \\ 
            &= \E_{S}\biggSquare{
                \sup_{f\in\mathcal{F}} {
                    \E_{S'}\biggSquare{
                        \frac{1}{n}\sum_{i=1}^n (f(Z_i) - f(Z_i'))
                    }
                }
            } \\
            &\le \E_{S, S'}\biggSquare{
                \sup_{f\in\mathcal{F}} {
                    \frac{1}{n}\sum_{i=1}^n (f(Z_i) - f(Z_i'))
                }
            } \quad (\text{Jensen's Inequality}) \\ 
            &= \E_{S, S', \sigma}\biggSquare{
                \sup_{f\in\mathcal{F}} {
                    \frac{1}{n}\sum_{i=1}^n \sigma_i(f(Z_i) - f(Z_i'))
                }
            } \quad (\sigma\sim\mathrm{Rad}^n, f(Z_i) - f(Z_i')\text{ is symmetric}) \\ 
            &\le \E_{S, \sigma}\biggSquare{\sup_{f\in\mathcal{F}}{\frac{1}{n}\sum_{i=1}^n \sigma_if(Z_i)}} + \E_{S', \sigma}\biggSquare{\sup_{f\in\mathcal{F}}{\frac{1}{n}\sum_{i=1}^n (-\sigma_i)f(Z_i')}} \\ 
            &= \E_{S, \sigma}\biggSquare{\sup_{f\in\mathcal{F}}{\frac{1}{n}\sum_{i=1}^n \sigma_if(Z_i)}} + \E_{S', \sigma}\biggSquare{\sup_{f\in\mathcal{F}}{\frac{1}{n}\sum_{i=1}^n \sigma_if(Z_i')}} \quad \text{(Rademacher variables are symmetric)} \\ 
            &= 2\E_{S, \sigma}\biggSquare{\sup_{f\in\mathcal{F}}{\frac{1}{n}\sum_{i=1}^n \sigma_if(Z_i)}} \quad \text{($S$ and $S'$ are identically distributed)}\\
            &= 2\mathfrak{R}_n(\mathcal{F}).
    \end{aligned}
    \end{equation}

    \noindent Using the same argument, we can also have:
    \begin{equation}
        \label{eq:ulln_eq2}
         \E_S\biggSquare{\sup_{f\in\mathcal{F}}{\frac{1}{n}\sum_{i=1}^n (\E_S[f(Z_i)] - f(Z_i))}} \le 2\mathfrak{R}_n(\mathcal{F}).
    \end{equation}

    \noindent Using equations \ref{eq:ulln_eq1} and \ref{eq:ulln_eq2} to bound $\Pm(\sup_{\theta\in\Theta}|\phi_\theta(S)|\ge \epsilon)$, we have:
    \begin{align*}
        &\Pm\Big(\sup_{\theta\in\Theta}|\phi_\theta(S)|\ge \epsilon\Big) \le \frac{4\mathfrak{R}_n(\mathcal{F}_\Theta)}{\epsilon}, 
        \text{ where }\mathcal{F}_\Theta = \bigCurl{
            f_\theta : \theta \in \Theta
        }.
    \end{align*}

    \noindent To complete the proof, we have to show that the Rademacher complexity $\mathfrak{R}_n(\mathcal{F}_\Theta) \to 0$ as $n\to\infty$. By the definition of Rademacher Complexity, we have:
    \begin{align*}
        \mathfrak{R}_n(\mathcal{F}_\Theta) = \E_{\bX\sim p_{\btheta}^n} \E_\sigma \bigSquare{
            \mathfrak{\hat R}_\bX(\mathcal{F}_\Theta)
        },
    \end{align*}

    \noindent It is sufficient to prove that the Empirical Rademacher Complexity $\mathfrak{\hat R}_\bX(\mathcal{F}_\Theta)\to0$ as $n\to\infty$. Using Dudley's Entropy Integral \cite[Lemma A.5]{article:bartlett2017}, we have:
    \begin{equation}
        \mathfrak{\hat R}_\bX(\mathcal{F}_\Theta) \le \frac{12}{\sqrt n}\int_\alpha^M \sqrt{\log\mathcal{N}\bigRound{\mathcal{F}_\Theta, \epsilon, L_2(\bX)}}d\epsilon, \quad \alpha > 0, 
    \end{equation}

    \noindent where $M<\infty$ is a constant such that $|f_\theta(x)| \le M$ $p_\btheta$-(almost everywhere) on $\mathcal{X}$ for all $\theta\in\Theta$\footnote{M exists because we have $\E_{\btheta}[\sup_{\theta\in\Theta}|f_\theta(X)|] < \infty$.}. Now, we need to construct a cover in $L_2$ norm for the class $\mathcal{F}_\Theta$. Since $f_\theta(x)$ is continuous in $\theta$ for all $x\in\mathcal{X}$, for all $\epsilon > 0$, there exists $\delta_x>0$ such that $\|\theta - \bar\theta\|_2<\delta_x \implies |f_\theta(x) - f_{\bar\theta}(x)|<\epsilon$. Define $\delta>0$ as follows:
    \begin{equation}
        \delta = \min_{1\le i \le n}\delta_{X_i}, \quad X_i \in \bX.
    \end{equation}

    \noindent Due to continuity, constructing an $\epsilon$-cover in $\mathcal{F}_\Theta$ is equivalent to constructing a $\delta$-cover for $\Theta$ with respect to the Euclidean norm. Since $\Theta$ is compact, there exists $N\in\mathbb{N}$ and a sequence of open balls $\{\mathcal{B}(y_j, r_j)\}_{j=1}^N$ where $y_j\in\Theta$ for all $1\le j \le N$ such that:
    \begin{equation}
        \Theta \subseteq \bigcup_{j=1}^N \mathcal{B}(y_j, r_j).
    \end{equation}

    \noindent By \cite[Lemma A.8]{article:long2020}, we have:
    \begin{equation}
        \log \mathcal{N}\bigRound{\mathcal{B}(y_j, r_j), \delta, \|.\|_2} \le m \log \biggRound{\frac{3r_j}{\delta}}.
    \end{equation}

    \noindent Therefore, we have:
    \begin{equation}
        \log\mathcal{N}\bigRound{\Theta, \delta, \|.\|_2} \le \sum_{j=1}^N \log \mathcal{N}\bigRound{\mathcal{B}(y_j, r_j), \delta, \|.\|_2} \le mN\log\biggRound{\frac{\prod_{j=1}^N r_j}{\delta^N}}.
    \end{equation}

    \noindent From the above covering number bound, we can see that $\log\mathcal{N}\bigRound{\Theta, \delta, \|.\|_2}$ does not grow with $n$ and therefore, $\log\mathcal{N}\bigRound{\mathcal{F}_\Theta, \epsilon, L_2(\bX)}$ does not grow with $n$. Therefore, we have $\mathfrak{\hat R}_{\bX}(\mathcal{F}_\Theta) \in \mathcal{O}(1/\sqrt{n})$ and $\mathfrak{\hat R}_{\bX}(\mathcal{F}_\Theta)\to 0$ as $n\to\infty$.
\end{proof*} 

\subsubsection{Central Limit Theorem}
\begin{theorem}{Central Limit Theorem $\CLT$}{central_limit_theorem}
    Let $X_1, \dots X_n$ be a sequence of $i.i.d$ random variables with expected value $\mu$ and finite variance $\sigma^2$. Then, we have:
    \begin{equation}
        \frac{\bar X_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1) \quad as \quad n \to \infty,
    \end{equation}

    \noindent where $\bar X_n = S_n/n$ and $\mathcal{N}(0,1)$ is the standard normal distribution.
\end{theorem}

\begin{proof*}[Central Limit Theorem $\CLT$]
    We prove this via the Characteristic Function. Let $\bar Z_n = \frac{\bar X_n - \mu}{\sigma/\sqrt n}$, notice that:
    \begin{align*}
        \bar Z_n &= \frac{\bar X_n - \mu}{\sigma/\sqrt n} = \frac{1}{\sqrt n}\sum_{i=1}^n \frac{X_i - \mu}{\sigma},
    \end{align*}

    \noindent Let $Z_i = X_i - \mu$ for $1\le i \le n$ and suppose $Z = Z_1 = \dots = Z_n$, we have:
    \begin{align*}
        \varphi_{\bar Z_n}(t) &= \varphi_{\sum_{i=1}^n Z_i}\biggRound{\frac{t}{\sqrt n}} = \biggSquare{
            \varphi_Z\biggRound{\frac{t}{\sqrt n}}
        }^n \\
        &= \biggSquare{
            1 + \frac{it\E[Z]}{\sqrt n} - \frac{t^2}{2n}\E[Z^2] + \mathcal{O}(1/n)
        }^n \quad \text{(Taylor's Expansion)} \\
        &= \biggSquare{
            1 - \frac{t^2}{2n} + \mathcal{O}(1/n)
        }^n. 
    \end{align*}

    \noindent The final equality comes from the fact that $\E[Z] = 0$ and $\E[Z^2] = \E[Z]^2 + \var(Z) = 1$. Finally, we have:
    \begin{align*}
        \lim_{n\to\infty}\varphi_{\bar Z_n}(t) = \lim_{n\to\infty} \biggSquare{
            1 - \frac{t^2}{2n} + \mathcal{O}(1/n)
        }^n = e^{-t^2/2}.
    \end{align*}

    \noindent Since $e^{-t^2/2}$ is the Characteristic Function of the standard normal distribution, by $\LCT$, we have $\bar Z_n \xrightarrow{d} \mathcal{N}(0,1)$.
\end{proof*}

